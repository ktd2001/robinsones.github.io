When I started at Etsy, I read a lot of papers and blog posts on A/B Testing. While I had strong background in statistics and experimentation, A/B Testing presents an additional set of problems. These are some of the ones that I found most helpful.  

- [Democraticizing online controlled experiments at Booking.com](https://arxiv.org/pdf/1710.08217.pdf): booking.com is definitely #goals of experimentation (they run more than 1,000 concurrent experiments at a time and have a team of 30), but this overview of their experimentation system provides you a great place to strive for. While you certainly won't be able to replicate everything they do, such as have two separate pipelines that compute experiment metrics, one in real-time and only daily, you can learn from their emphasis on building trust in the infrastructure and data quality, maintaining a pool of A/A experiments (where there's no difference) to surface issues, enforcing good practices of pre-registration and statistical checks.
- [From Infrastructure to Culture: A/B Testing Challenges in Large Scale Social Networks](https://content.linkedin.com/content/dam/engineering/site-assets/pdfs/ABTestingSocialNetwork_share.pdf): while the first section deals with the complications of overlapping experiments and social networks, the latter sections have great rules of thumbs for simplifying the multiple testing problem and allowing metric owners to follow other people's experiments that are impacting their metric. 
- [Online Experimentation at Microsoft](http://ai.stanford.edu/~ronnyk/ExPThinkWeek2009Public.pdf): an accessible overview of how Microsoft started A/B Testing, tackling the issues of how most experiments fail, how they spread awareness, and lessons learned. 
- [Controlled Experiments on the Web: Survey and Practical Guide](https://ai.stanford.edu/~ronnyk/2009controlledExperimentsOnTheWebSurvey.pdf): an essential guide to best practices, some of which are covered in my rules above. 
- [Seven Rules of Thumb for Web Site Experiments](https://www.exp-platform.com/Documents/2014%20experimentersRulesOfThumb.pdf): while the details can be complicated, the seven rules are clear: it's easy to increase clicks on a feature but not for the whole page, be critical of your "too good to be true" results, 
- [Selection Bias in Online Experimentation](https://medium.com/airbnb-engineering/selection-bias-in-online-experimentation-c3d67795cceb) or why all your wins aren't adding up. 

### Next Time 

I've been working at DataCamp for about four months now. One of the main reasons they hired me was to build out their experimentation system. It's a whole different set of challenges and mindset. I'll be giving a talk on it in October at the [Noreast'R conference](http://noreastrconf.com/) about building an A/B Testing system. 
